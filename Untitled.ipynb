{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skdim import global_id\n",
    "from skdim import local_id\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "utils = rpackages.importr('utils')\n",
    "#utils.install_packages('intrinsicDimension')\n",
    "#utils.install_packages('ider')\n",
    "intdimr = rpackages.importr('intrinsicDimension')\n",
    "ider   = rpackages.importr('ider')\n",
    "r_base = rpackages.importr('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def pcaOtpmPointwiseDimEst(data, N, alpha = 0.05):\n",
    "    km = KMeans(n_clusters=N)\n",
    "    km.fit(data)\n",
    "    pt = km.cluster_centers_\n",
    "    pt_bm = km.labels_\n",
    "    pt_sm = np.repeat(np.nan, len(pt_bm))\n",
    "\n",
    "    for k in range(len(data)):\n",
    "        pt_sm[k] = np.argmin(lens(pt[[i for i in range(N) if i!=pt_bm[k]],:] - data[k,:]))\n",
    "        if (pt_sm[k] >= pt_bm[k]):\n",
    "            pt_sm[k] += 1\n",
    "\n",
    "    de_c = np.repeat(np.nan, N)\n",
    "    nbr_nb_c = np.repeat(np.nan, N)\n",
    "    for k in range(N):\n",
    "        nb = np.unique(np.concatenate((pt_sm[pt_bm == k], pt_bm[pt_sm == k]))).astype(int)\n",
    "        nbr_nb_c[k] = len(nb)\n",
    "        loc_dat = pt[nb,:] - pt[k,:]\n",
    "        if len(loc_dat) == 1:\n",
    "            continue\n",
    "        de_c[k] = lPCA().fit(loc_dat).dimension_ #pcaLocalDimEst(loc_dat, ver = \"FO\", alphaFO = alpha)\n",
    "\n",
    "    de = de_c[pt_bm]\n",
    "    nbr_nb = nbr_nb_c[pt_bm]\n",
    "    return de, nbr_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]  4 10\n",
      "[1]  5 10\n",
      "[1]  3 10\n",
      "[1]  7 10\n",
      "[1]  9 10\n",
      "[1]  6 10\n",
      "[1]  7 10\n",
      "[1]  5 10\n",
      "[1]  3 10\n",
      "[1]  4 10\n",
      "[1]  8 10\n",
      "[1]  4 10\n",
      "[1]  5 10\n",
      "[1]  8 10\n",
      "[1]  6 10\n",
      "[1]  9 10\n",
      "[1]  3 10\n",
      "[1]  5 10\n",
      "[1]  9 10\n",
      "[1]  5 10\n",
      "[1]  4 10\n",
      "[1] 11 10\n",
      "[1]  8 10\n",
      "[1]  7 10\n",
      "[1]  4 10\n",
      "[1]  7 10\n",
      "[1]  4 10\n",
      "[1]  8 10\n",
      "[1]  9 10\n",
      "[1]  5 10\n",
      "[1]  6 10\n",
      "[1]  8 10\n",
      "[1] 10 10\n",
      "[1] 11 10\n",
      "[1]  5 10\n",
      "[1]  9 10\n",
      "[1]  5 10\n",
      "[1]  8 10\n",
      "[1]  6 10\n",
      "[1]  3 10\n",
      "[1]  6 10\n",
      "[1]  3 10\n",
      "[1]  5 10\n",
      "[1]  7 10\n",
      "[1] 13 10\n",
      "[1]  3 10\n",
      "[1]  5 10\n",
      "[1] 13 10\n",
      "[1]  8 10\n",
      "[1]  2 10\n"
     ]
    }
   ],
   "source": [
    "%%R -i data -i N\n",
    "\n",
    "alpha <- .05\n",
    "\n",
    "lens <- function(vectors) \n",
    "  sqrt(apply(vectors^2, 1, sum))\n",
    "\n",
    "km <- kmeans(data, N)\n",
    "pt <- km$centers\n",
    "pt_bm <- km$cluster\n",
    "pt_sm <- rep(NA, length(pt_bm))\n",
    "for (k in 1:dim(data)[1]) {\n",
    "    pt_sm[k] <- which.min(lens(rowSubtract(pt[-pt_bm[k], \n",
    "        ], data[k, ])))\n",
    "    if (pt_sm[k] >= pt_bm[k]) \n",
    "        pt_sm[k] <- pt_sm[k] + 1\n",
    "}\n",
    "de.c <- rep(NA, N)\n",
    "nbr.nb.c <- rep(NA, N)\n",
    "for (k in 1:N) {\n",
    "    nb <- unique(c(pt_sm[pt_bm == k], pt_bm[pt_sm == k]))\n",
    "    nbr.nb.c[k] <- length(nb)\n",
    "    loc.dat <- rowSubtract(pt[nb, ], pt[k, ])\n",
    "    print(dim(loc.dat))\n",
    "    de.c[k] <- pcaLocalDimEst(loc.dat, ver = \"FO\", alphaFO = alpha)$dim.est\n",
    "}\n",
    "de <- de.c[pt_bm]\n",
    "nbr.nb <- nbr.nb.c[pt_bm]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pca = lPCA().fit(data).dimension_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "\n",
    "class lPCA(BaseEstimator):    \n",
    "    \"\"\" Intrinsic dimension estimation using local PCA.\n",
    "    Version 'FO' is the method by Fukunaga-Olsen, version 'fan' is the method by Fan et al..\n",
    "    Version 'maxgap' returns the position of the largest relative gap in the sequence of singular values.\n",
    "    All versions assume that the data is local, i.e. that it is a neighborhood taken from a larger data set, such that the curvature and the noise within the neighborhood is relatively small. In the ideal case (no noise, no curvature) this is equivalent to the data being uniformly distributed over a hyper ball.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ver : str \t\n",
    "        Version. Possible values: 'FO', 'fan', 'maxgap'.\n",
    "    alphaRatio : float\n",
    "        Only for ver = 'ratio'. Intrinsic dimension is estimated to be the number of principal components needed to retain (1-alphaRatio) percent of the variance.\n",
    "    alphaFO: float\n",
    "        Only for ver = 'FO'. An eigenvalue is considered significant if it is larger than alpha times the largest eigenvalue.\n",
    "    alphaFan : float\n",
    "        Only for ver = 'Fan'. The alpha parameter (large gap threshold).\n",
    "    betaFan : float\n",
    "        Only for ver = 'Fan'. The beta parameter (total covariance threshold).\n",
    "    PFan : float\n",
    "        Only for ver = 'Fan'. Total covariance in non-noise.\n",
    "    verbose : bool, default=False\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Fukunaga, K. and Olsen, D. R. (1971). An algorithm for finding intrinsic dimensionality of data. IEEE Trans. Comput., c-20(2):176-183.\n",
    "    \n",
    "    Fan, M. et al. (2010). Intrinsic dimension estimation of data by principal component analysis. arXiv preprint 1002.2050. \n",
    "    \"\"\"\n",
    "    def __init__(self, ver = 'FO', alphaRatio = .05, alphaFO = .05, alphaFan = 10, betaFan = .8,\n",
    "                 PFan = .95, verbose = True):\n",
    "        \n",
    "        self.ver = ver\n",
    "        self.alphaRatio = alphaRatio\n",
    "        self.alphaFO = alphaFO\n",
    "        self.alphaFan = alphaFan\n",
    "        self.betaFan = betaFan\n",
    "        self.PFan = PFan\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self,X):\n",
    "        \"\"\"A reference implementation of a fitting function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            A local dataset of training input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        self.dimension_, self.gap_ = self._pcaLocalDimEst(X)\n",
    "        self.is_fitted_ = True\n",
    "        # `fit` should always return `self`\n",
    "        return self        \n",
    "\n",
    "\n",
    "    def _pcaLocalDimEst(self, X):\n",
    "        pca = PCA().fit(X)\n",
    "        explained_var = pca.explained_variance_    \n",
    "\n",
    "        if (self.ver == 'FO'): return(self._FO(explained_var))\n",
    "        elif (self.ver == 'fan'): return(self._fan(explained_var))\n",
    "        elif (self.ver == 'maxgap'): return(self._maxgap(explained_var))\n",
    "\n",
    "    def _FO(self,explained_var):\n",
    "        de = sum(explained_var>(self.alphaFO*explained_var[0]))\n",
    "        gaps = explained_var[:-1]/explained_var[1:]\n",
    "        try: return de, gaps[de-1]\n",
    "        except: return de, gaps[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _maxgap(explained_var):\n",
    "        gaps = explained_var[:-1]/explained_var[1:]\n",
    "        de = np.argmax(gaps)+1\n",
    "        try: return de, gaps[de-1]\n",
    "        except: return de, gaps[-1]\n",
    "        \n",
    "        \n",
    "    def _ratio(self, explained_var):\n",
    "        #X - data set (n x d)\n",
    "        #theta - ratio of variance to preserve (theta \\in [0,1])\n",
    "        sumexp = np.cumsum(explained_var)\n",
    "        de = np.where(sumexp>(1-self.alphaRatio))[0].min() + 1 \n",
    "        return de\n",
    "        \n",
    "    def _fan(self,explained_var):\n",
    "        r = np.where(np.cumsum(explained_var)/sum(explained_var) > self.PFan)[0][0]\n",
    "        sigma = np.mean(explained_var[r:])\n",
    "        explained_var -= sigma\n",
    "        gaps = explained_var[:-1]/explained_var[1:]\n",
    "        de = 1 + np.min(np.concatenate((np.where(gaps>self.alphaFan)[0],\n",
    "                                    np.where((np.cumsum(explained_var)/sum(explained_var))>self.betaFan)[0])))\n",
    "        try: return de, gaps[de-1]\n",
    "        except: return de, gaps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.random.random((500,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Computing DANCo calibration data for N = 500, k = 10 for dimensions 1 to 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = intdimr.essLocalDimEst(data)\n",
    "r_ess = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.pcaLocalDimEst(data,ver='FO')\n",
    "r_pca = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.dancoDimEst(data,k=10,D=10)\n",
    "r_danco = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.knnDimEst(data,k=10,ps=np.arange(11,50),M=5,gamma=2)\n",
    "r_knn = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.maxLikGlobalDimEst(data,k=20)\n",
    "r_gmle = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.maxLikLocalDimEst(data)\n",
    "r_lmle = dict(zip(res.names,[np.array(i) for i in res]))\n",
    "\n",
    "res = intdimr.maxLikPointwiseDimEst(data,k=20)\n",
    "r_pmle = np.array([i[0] for i in res])\n",
    "\n",
    "res = intdimr.pcaOtpmPointwiseDimEst(data,N=50)\n",
    "r_pcaOtpm = [np.array([i[0] for i in res]), np.array([i[1] for i in res])]\n",
    "\n",
    "\n",
    "r_corint = np.array(ider.corint(data,k1=10,k2=20))\n",
    "r_mada = np.array(ider.mada(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corint = global_id.CorrInt().fit(data)\n",
    "danco = global_id.DANCo().fit(data)\n",
    "knn = global_id.KNN().fit(data)\n",
    "mada = global_id.Mada().fit(data)\n",
    "twonn = global_id.TwoNN().fit(data)\n",
    "\n",
    "ess = local_id.ESS().fit(data)\n",
    "fishers = local_id.FisherS().fit(data)\n",
    "lpca = local_id.lPCA().fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension estimate: 6.312511 \n"
     ]
    }
   ],
   "source": [
    "%%R -i data\n",
    "r=maxLikGlobalDimEst(data,k=10)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import scipy.integrate\n",
    "import numpy as np\n",
    "from _commonfuncs import lens, get_nn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "\n",
    "class MLE(BaseEstimator):    \n",
    "    \"\"\"Maximum likelihood estimator of intrinsic dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    demo_param : str, default='demo_param'\n",
    "        A parameter used for demonstation of how to pass and store paramters.\n",
    "    \"\"\"\n",
    "    def __init__(self, k, dnoise = None, sigma = 0, n = None,\n",
    "                    integral_approximation = 'Haro', unbiased = False,\n",
    "                    neighborhood_based = True,\n",
    "                    neighborhood_aggregation = 'maximum.likelihood',\n",
    "                    iterations = 5, K = 5):\n",
    "        \n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "    def fit(self,X):\n",
    "        \"\"\"A reference implementation of a fitting function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        self.dimension_, self.kl_divergence_, self.calibration_data_ = self._dancoDimEst(X)\n",
    "        self.is_fitted_ = True\n",
    "        # `fit` should always return `self`\n",
    "        return self    \n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def maxLikGlobalDimEst(data, k, dnoise = None, sigma = 0, n = None,\n",
    "                        integral_approximation = 'Haro', unbiased = False,\n",
    "                        neighborhood_based = True,\n",
    "                        neighborhood_aggregation = 'maximum.likelihood',\n",
    "                        iterations = 5, K = 5):\n",
    "      # 'k' is the number of neighbors used for each dimension estimation.\n",
    "      # 'dnoise' is a vector valued function giving the transition density.\n",
    "      # 'sigma' is the estimated standard deviation for the noise.\n",
    "      # 'n' is the dimension of the noise (at least dim(data)[2])\n",
    "      # integral.approximation can take values 'Haro', 'guaranteed.convergence', 'iteration'\n",
    "      # neighborhood.based means that estimation is made for each neighborhood,\n",
    "      # otherwise estimation is based on distances in entire data set.\n",
    "      # 'K' is number of neighbors per data point that is considered, only used for \n",
    "      # neighborhood.based = False\n",
    "\n",
    "        N = len(data)\n",
    "\n",
    "        if (neighborhood_based):\n",
    "            mi = maxLikPointwiseDimEst(data, k, dnoise, sigma, n,\n",
    "                               integral_approximation = integral_approximation,\n",
    "                               unbiased = unbiased)\n",
    "\n",
    "            if neighborhood_aggregation == 'maximum.likelihood':\n",
    "                de = 1/np.mean(1/mi) \n",
    "\n",
    "            elif neighborhood_aggregation == 'mean':\n",
    "                de = np.mean(mi) \n",
    "            elif neighborhood_aggregation == 'median':\n",
    "                de = robust = np.median(mi)\n",
    "            return(de)\n",
    "\n",
    "\n",
    "        dist, idx = get_nn(data, K)\n",
    "        #dist = dist[~duplicated(dist[range(K*N)])]\n",
    "        Rs = dist\n",
    "        de = maxLikDimEstFromR(Rs, dnoise, np.sqrt(2)*sigma, n, integral_approximation, unbiased,\n",
    "                iterations) # Since distances between points are used, noise is \n",
    "                            # added at both ends, i.e. variance is doubled. \n",
    "\n",
    "                  #likelihood = np.nan\n",
    "        return(de, np.nan) \n",
    "\n",
    "\n",
    "    def maxLikPointwiseDimEst(data, k, dnoise = None, sigma = 0, n = None, indices = None,\n",
    "                             integral_approximation = 'Haro', unbiased = False,\n",
    "                             iterations = 5):\n",
    "        ## estimates dimension around each point in data[indices, ]\n",
    "        #\n",
    "        # 'indices' give the indexes for which local dimension estimation should\n",
    "        # be performed.\n",
    "        # 'k' is the number of neighbors used for each local dimension estimation.\n",
    "        # 'dnoise' is a vector valued function giving the transition density.\n",
    "        # 'sigma' is the estimated standard deviation for the noise.\n",
    "        # 'n' is the dimension of the noise (at least dim(data)[2])\n",
    "\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(data))\n",
    "\n",
    "        N = len(indices)\n",
    "        nbh_dist, idx = get_nn(data, k)\n",
    "        de = np.repeat(np.nan, N) # This vector will hold local dimension estimates\n",
    "\n",
    "        for i in range(N):\n",
    "            Rs = nbh_dist[i,:]\n",
    "            de[i] = maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation, unbiased, iterations)\n",
    "\n",
    "        return(de)\n",
    "\n",
    "\n",
    "    def maxLikLocalDimEst(data, dnoise = None, sigma = 0, n = None,\n",
    "                       integral_approximation = 'Haro',\n",
    "                       unbiased = False, iterations = 5):\n",
    "\n",
    "        # assuming data set is local\n",
    "        center = np.mean(data, axis=0)\n",
    "        cent_data = data - center\n",
    "        Rs = np.sort(lens(cent_data))\n",
    "        de = maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation, unbiased, iterations)\n",
    "        return(de)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation = 'Haro',\n",
    "                    unbiased = False, iterations = 5):\n",
    "\n",
    "        if integral_approximation not in ['Haro', 'guaranteed.convergence', 'iteration']:\n",
    "            raise ValueError('Wrong integral approximation')\n",
    "\n",
    "        if not type(dnoise) == 'closure' and dnoise is not None: \n",
    "            if not callable(dnoise):\n",
    "                raise ValueError('dnoise must be a function')\n",
    "            dnoise_orig = dnoise\n",
    "        if not integral_approximation == 'Haro' and dnoise is not None:\n",
    "            dnoise = lambda r, s, sigma, k: r*dnoise_orig(r, s, sigma, k)\n",
    "\n",
    "        de = maxLikDimEstFromR_haro_approx(Rs, dnoise, sigma, n, unbiased)\n",
    "        if (integral_approximation == 'iteration'):\n",
    "            de = maxLikDimEstFromRIterative(Rs, dnoise_orig, sigma, n, de, unbiased)\n",
    "\n",
    "        return(de)\n",
    "    ################################################################################\n",
    "\n",
    "    def maxLikDimEstFromR_haro_approx(Rs, dnoise, sigma, n, unbiased = False):\n",
    "        # if dnoise is the noise function this is the approximation used in Haro.\n",
    "        # for 'guaranteed.convergence' dnoise should be r times the noise function\n",
    "        # with 'unbiased' option, estimator is unbiased if no noise or boundary\n",
    "\n",
    "        k = len(Rs)\n",
    "        kfac = k-2 if unbiased else k-1\n",
    "\n",
    "        Rk = max(Rs)\n",
    "        if dnoise is None:\n",
    "            return(kfac/(sum(np.log(Rk/Rs))))\n",
    "\n",
    "        Rpr = Rk + 100*sigma\n",
    "\n",
    "        numerator = np.repeat(np.nan, k - 1)\n",
    "        denominator = np.repeat(np.nan, k - 1)\n",
    "\n",
    "        for j in range(k-1):\n",
    "            Rj = Rs[j]\n",
    "\n",
    "            try:\n",
    "                tc = scipy.integrate.quad(lambda x: dnoise(x, Rj, sigma, n) * np.log(Rk/x), 0, Rpr, epsrel = 1e-2)\n",
    "            except:\n",
    "                tc = np.nan\n",
    "\n",
    "            if np.isnan(tc)[0]:\n",
    "                return(np.nan)\n",
    "\n",
    "            numerator[j] = numInt['value']\n",
    "\n",
    "            try:\n",
    "                tc = scipy.integrate.quad(lambda x: dnoise(x, Rj, sigma, n), 0, Rpr, epsrel = 1e-2)\n",
    "            except:\n",
    "                tc = np.nan\n",
    "\n",
    "            if np.isnan(tc)[0]:\n",
    "                return(np.nan)\n",
    "\n",
    "            denominator[j] = denomInt['value']\n",
    "\n",
    "        return(kfac/sum(numerator/denominator))\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def maxLikDimEstFromRIterative(Rs, dnoise, sigma, n, init = 5,\n",
    "                      unbiased = False, iterations = 5, verbose = False):\n",
    "        m = init\n",
    "        if verbose:\n",
    "            print(\"Start iteration, intial value:\", m, \"\\n\")\n",
    "        for i in range(iterations):\n",
    "            m = maxLikDimEstFromRIterative_inner(Rs, dnoise, sigma, n, m, unbiased)\n",
    "            if verbose:\n",
    "                print(\"Iteration\", i, \":\", m, \"\\n\")\n",
    "            if verbose:\n",
    "                print(\"\\n\")\n",
    "        return(m)\n",
    "\n",
    "    def maxLikDimEstFromRIterative_inner(Rs, dnoise, sigma, n, m, unbiased):\n",
    "\n",
    "        k = len(Rs)  \n",
    "        kfac = k-2 if unbiased else k-1\n",
    "\n",
    "        Rk = max(Rs)\n",
    "        if dnoise is None:\n",
    "            return(kfac/(sum(np.log(Rk/Rs))))\n",
    "        Rpr = Rk + 100*sigma\n",
    "\n",
    "        numerator = np.repeat(np.nan, k - 1)\n",
    "        denominator = np.repeat(np.nan, k - 1)\n",
    "\n",
    "        for j in range(k-1):\n",
    "            Rj = Rs[j]\n",
    "            m = max(m, 1)\n",
    "            numInt = scipy.integrate.quad(lambda x: x**(m-1)*dnoise(x, Rj, sigma, n) * np.log(Rk/x), 0, Rpr, epsrel = 1e-2)\n",
    "            numerator[j] = numInt['value']\n",
    "\n",
    "            denomInt = scipy.integrate.quad(lambda x: x**(m-1)*dnoise(x, Rj, sigma, n), 0, Rpr, epsrel = 1e-2)\n",
    "            denominator[j] = denomInt['value']\n",
    "\n",
    "        return(kfac/sum(numerator/denominator))\n",
    "\n",
    "\n",
    "\n",
    "    def dnoiseGaussH(r, s, sigma, k = None):\n",
    "        if (len(r) > 1 and len(s) > 1):\n",
    "            raise ValueError('r and s cannot both be vectors')\n",
    "        dnorm(s, r, sigma)              # f(s|r) in Haro et al. (2008) w/ Gaussian\n",
    "                                            # transition density\n",
    "                                            # 'k' is not used, but is input\n",
    "                                            # for compatibility\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def dnoiseGaussB(r, s, sigma, k):\n",
    "\n",
    "        if (len(r) > 1 and len(s) > 1):\n",
    "            raise ValueError('r and s cannot both be vectors')\n",
    "        w1 = inthr(s, k, sigma)\n",
    "        mu = intrhr(s, k, sigma)/w1\n",
    "        tsigma2 = intr2hr(s, k, sigma)/w1 - mu^2\n",
    "        w2 = 1 - pnorm(0, mean = mu, sd = sqrt(tsigma2))\n",
    "        w = w1/w2\n",
    "        gau = w*dnorm(r, mu, sqrt(tsigma2))\n",
    "        return(gau*(r > 0))   # Best approximation of f(s|r) by truncated Gaussian\n",
    "                            # when Gaussian k-dim noise.\n",
    "                            # NB! The noncentral chi distribution is\n",
    "                            # f(r|s), not f(s|r).\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def dnoiseNcChi(r, s, sigma, k):\n",
    "\n",
    "        if len(r) > 1 and len(s) > 1:\n",
    "            raise ValueError('r and s cannot both be vectors')\n",
    "        _lambda = r/sigma\n",
    "        return 2*s/sigma**2*dchisq((s/sigma)**2, k, _lambda**2) # 'k' is the number of\n",
    "                                                     # degrees of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "import numpy as np\n",
    "from _commonfuncs import lens, get_nn\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def maxLikGlobalDimEst(data, k, dnoise = None, sigma = 0, n = None,\n",
    "                    integral_approximation = 'Haro', unbiased = False,\n",
    "                    neighborhood_based = True,\n",
    "                    neighborhood_aggregation = 'maximum.likelihood',\n",
    "                    iterations = 5, K = 5):\n",
    "  # 'k' is the number of neighbors used for each dimension estimation.\n",
    "  # 'dnoise' is a vector valued function giving the transition density.\n",
    "  # 'sigma' is the estimated standard deviation for the noise.\n",
    "  # 'n' is the dimension of the noise (at least dim(data)[2])\n",
    "  # integral.approximation can take values 'Haro', 'guaranteed.convergence', 'iteration'\n",
    "  # neighborhood.based means that estimation is made for each neighborhood,\n",
    "  # otherwise estimation is based on distances in entire data set.\n",
    "  # 'K' is number of neighbors per data point that is considered, only used for \n",
    "  # neighborhood.based = False\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    if (neighborhood_based):\n",
    "        mi = maxLikPointwiseDimEst(data, k, dnoise, sigma, n,\n",
    "                           integral_approximation = integral_approximation,\n",
    "                           unbiased = unbiased)\n",
    "        \n",
    "        if neighborhood_aggregation == 'maximum.likelihood':\n",
    "            de = 1/np.mean(1/mi) \n",
    "            \n",
    "        elif neighborhood_aggregation == 'mean':\n",
    "            de = np.mean(mi) \n",
    "        elif neighborhood_aggregation == 'median':\n",
    "            de = robust = np.median(mi)\n",
    "        return(de)\n",
    "\n",
    "\n",
    "    dist, idx = get_nn(data, K)\n",
    "    #dist = dist[~duplicated(dist[range(K*N)])]\n",
    "    Rs = dist\n",
    "    de = maxLikDimEstFromR(Rs, dnoise, np.sqrt(2)*sigma, n, integral_approximation, unbiased,\n",
    "            iterations) # Since distances between points are used, noise is \n",
    "                        # added at both ends, i.e. variance is doubled. \n",
    "        \n",
    "              #likelihood = np.nan\n",
    "    return(de, np.nan) \n",
    "\n",
    "\n",
    "def maxLikPointwiseDimEst(data, k, dnoise = None, sigma = 0, n = None, indices = None,\n",
    "                         integral_approximation = 'Haro', unbiased = False,\n",
    "                         iterations = 5):\n",
    "    ## estimates dimension around each point in data[indices, ]\n",
    "    #\n",
    "    # 'indices' give the indexes for which local dimension estimation should\n",
    "    # be performed.\n",
    "    # 'k' is the number of neighbors used for each local dimension estimation.\n",
    "    # 'dnoise' is a vector valued function giving the transition density.\n",
    "    # 'sigma' is the estimated standard deviation for the noise.\n",
    "    # 'n' is the dimension of the noise (at least dim(data)[2])\n",
    "\n",
    "    if indices is None:\n",
    "        indices = np.arange(len(data))\n",
    "\n",
    "    N = len(indices)\n",
    "    nbh_dist, idx = get_nn(data, k)\n",
    "    de = np.repeat(np.nan, N) # This vector will hold local dimension estimates\n",
    "\n",
    "    for i in range(N):\n",
    "        Rs = nbh_dist[i,:]\n",
    "        de[i] = maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation, unbiased, iterations)\n",
    "\n",
    "    return(de)\n",
    "\n",
    "\n",
    "def maxLikLocalDimEst(data, dnoise = None, sigma = 0, n = None,\n",
    "                   integral_approximation = 'Haro',\n",
    "                   unbiased = False, iterations = 5):\n",
    "    \n",
    "    # assuming data set is local\n",
    "    center = np.mean(data, axis=0)\n",
    "    cent_data = data - center\n",
    "    Rs = np.sort(lens(cent_data))\n",
    "    de = maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation, unbiased, iterations)\n",
    "    return(de)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def maxLikDimEstFromR(Rs, dnoise, sigma, n, integral_approximation = 'Haro',\n",
    "                unbiased = False, iterations = 5):\n",
    "\n",
    "    if integral_approximation not in ['Haro', 'guaranteed.convergence', 'iteration']:\n",
    "        raise ValueError('Wrong integral approximation')\n",
    "\n",
    "    if not type(dnoise) == 'closure' and dnoise is not None: \n",
    "        if not callable(dnoise):\n",
    "            raise ValueError('dnoise must be a function')\n",
    "        dnoise_orig = dnoise\n",
    "    if not integral_approximation == 'Haro' and dnoise is not None:\n",
    "        dnoise = lambda r, s, sigma, k: r*dnoise_orig(r, s, sigma, k)\n",
    "\n",
    "    de = maxLikDimEstFromR_haro_approx(Rs, dnoise, sigma, n, unbiased)\n",
    "    if (integral_approximation == 'iteration'):\n",
    "        de = maxLikDimEstFromRIterative(Rs, dnoise_orig, sigma, n, de, unbiased)\n",
    "\n",
    "    return(de)\n",
    "################################################################################\n",
    "\n",
    "def maxLikDimEstFromR_haro_approx(Rs, dnoise, sigma, n, unbiased = False):\n",
    "    # if dnoise is the noise function this is the approximation used in Haro.\n",
    "    # for 'guaranteed.convergence' dnoise should be r times the noise function\n",
    "    # with 'unbiased' option, estimator is unbiased if no noise or boundary\n",
    "\n",
    "    k = len(Rs)\n",
    "    kfac = k-2 if unbiased else k-1\n",
    "\n",
    "    Rk = max(Rs)\n",
    "    if dnoise is None:\n",
    "        return(kfac/(sum(np.log(Rk/Rs))))\n",
    "\n",
    "    Rpr = Rk + 100*sigma\n",
    "\n",
    "    numerator = np.repeat(np.nan, k - 1)\n",
    "    denominator = np.repeat(np.nan, k - 1)\n",
    "\n",
    "    for j in range(k-1):\n",
    "        Rj = Rs[j]\n",
    "        \n",
    "        try:\n",
    "            tc = scipy.integrate.quad(lambda x: dnoise(x, Rj, sigma, n) * np.log(Rk/x), 0, Rpr, epsrel = 1e-2)\n",
    "        except:\n",
    "            tc = np.nan\n",
    "\n",
    "        if np.isnan(tc)[0]:\n",
    "            return(np.nan)\n",
    "\n",
    "        numerator[j] = numInt['value']\n",
    "\n",
    "        try:\n",
    "            tc = scipy.integrate.quad(lambda x: dnoise(x, Rj, sigma, n), 0, Rpr, epsrel = 1e-2)\n",
    "        except:\n",
    "            tc = np.nan\n",
    "\n",
    "        if np.isnan(tc)[0]:\n",
    "            return(np.nan)\n",
    "        \n",
    "        denominator[j] = denomInt['value']\n",
    "\n",
    "    return(kfac/sum(numerator/denominator))\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def maxLikDimEstFromRIterative(Rs, dnoise, sigma, n, init = 5,\n",
    "                  unbiased = False, iterations = 5, verbose = False):\n",
    "    m = init\n",
    "    if verbose:\n",
    "        print(\"Start iteration, intial value:\", m, \"\\n\")\n",
    "    for i in range(iterations):\n",
    "        m = maxLikDimEstFromRIterative_inner(Rs, dnoise, sigma, n, m, unbiased)\n",
    "        if verbose:\n",
    "            print(\"Iteration\", i, \":\", m, \"\\n\")\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "    return(m)\n",
    "\n",
    "def maxLikDimEstFromRIterative_inner(Rs, dnoise, sigma, n, m, unbiased):\n",
    "\n",
    "    k = len(Rs)  \n",
    "    kfac = k-2 if unbiased else k-1\n",
    "\n",
    "    Rk = max(Rs)\n",
    "    if dnoise is None:\n",
    "        return(kfac/(sum(log(Rk/Rs))))\n",
    "    Rpr = Rk + 100*sigma\n",
    "\n",
    "    numerator = np.repeat(np.nan, k - 1)\n",
    "    denominator = np.repeat(np.nan, k - 1)\n",
    "\n",
    "    for j in range(k-1):\n",
    "        Rj = Rs[j]\n",
    "        m = max(m, 1)\n",
    "        numInt = scipy.integrate.quad(lambda x: x**(m-1)*dnoise(x, Rj, sigma, n) * np.log(Rk/x), 0, Rpr, epsrel = 1e-2)\n",
    "        numerator[j] = numInt['value']\n",
    "\n",
    "        denomInt = scipy.integrate.quad(lambda x: x**(m-1)*dnoise(x, Rj, sigma, n), 0, Rpr, epsrel = 1e-2)\n",
    "        denominator[j] = denomInt['value']\n",
    "\n",
    "    return(kfac/sum(numerator/denominator))\n",
    "\n",
    "\n",
    "\n",
    "def dnoiseGaussH(r, s, sigma, k = None):\n",
    "    if (len(r) > 1 and len(s) > 1):\n",
    "        raise ValueError('r and s cannot both be vectors')\n",
    "    dnorm(s, r, sigma)              # f(s|r) in Haro et al. (2008) w/ Gaussian\n",
    "                                        # transition density\n",
    "                                        # 'k' is not used, but is input\n",
    "                                        # for compatibility\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def dnoiseGaussB(r, s, sigma, k):\n",
    "\n",
    "    if (len(r) > 1 and len(s) > 1):\n",
    "        raise ValueError('r and s cannot both be vectors')\n",
    "    w1 = inthr(s, k, sigma)\n",
    "    mu = intrhr(s, k, sigma)/w1\n",
    "    tsigma2 = intr2hr(s, k, sigma)/w1 - mu^2\n",
    "    w2 = 1 - pnorm(0, mean = mu, sd = sqrt(tsigma2))\n",
    "    w = w1/w2\n",
    "    gau = w*dnorm(r, mu, sqrt(tsigma2))\n",
    "    return(gau*(r > 0))   # Best approximation of f(s|r) by truncated Gaussian\n",
    "                        # when Gaussian k-dim noise.\n",
    "                        # NB! The noncentral chi distribution is\n",
    "                        # f(r|s), not f(s|r).\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def dnoiseNcChi(r, s, sigma, k):\n",
    "\n",
    "    if len(r) > 1 and len(s) > 1:\n",
    "        raise ValueError('r and s cannot both be vectors')\n",
    "    _lambda = r/sigma\n",
    "    return 2*s/sigma**2*dchisq((s/sigma)**2, k, _lambda**2) # 'k' is the number of\n",
    "                                                 # degrees of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
